{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from copy import copy\n",
    "import sklearn.datasets\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = sklearn.datasets.make_hastie_10_2()\n",
    "X_train = X[0:8000,:]\n",
    "y_train = y[0:8000]\n",
    "X_test = X[8000:,:]\n",
    "y_test = y[8000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Implement the AdaBoost ensemble algorithm by completing the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn.base import clone\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, weakModel, T):\n",
    "        #genero T modelli\n",
    "        self.weakModels = list()        \n",
    "        self.weakModels.append(weakModel)\n",
    "        for i in range(0, T):\n",
    "            self.weakModels.append(copy(weakModel))\n",
    "        self.T = T\n",
    "        #array pesi modelli\n",
    "        self.cw = np.array([])\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        w = np.ones(len(X)) / len(X)\n",
    "\n",
    "        for t in range(0, self.T):\n",
    "            self.weakModels[t].fit(X,y,sample_weight=w*len(X)) #alleno il modello t-esimo\n",
    "            m = self._weakPredict(X,t)\n",
    "\n",
    "            miss = [int(x) for x in (m != y)]\n",
    "            miss2 = [x if x==1 else -1 for x in miss]\n",
    "\n",
    "            err = np.dot(w,miss) / sum(w)\n",
    "\n",
    "            #err = AdaBoost._computeErr(w,m,y)/sum(w) #calcolo errore pesato\n",
    "            self.cw = np.append(self.cw,(0.5*math.log((1-err)/err))) #calcolo peso classificatore\n",
    "\n",
    "            w = self._updateWeights(w,m,y,t) #aggiorno i pesi per gli esempi in base alla classificazione\n",
    "\n",
    "            #self.weakModels.append(type(self.weakModels[t])(self.param))\n",
    "            if(t % math.floor(self.T/10) == 0):\n",
    "                print(\"Iteration T: {} weighted err:{} Classification Error:{}\".format(t, err, AdaBoost.classificationErr(self.predictT(X,t),y)))\n",
    "\n",
    "    def predictT(self, X, T):\n",
    "        y = np.array([])\n",
    "        for x in X:\n",
    "            s = 0\n",
    "            for t in range(0, T):\n",
    "                s += self.weakModels[t].predict(x.reshape(1,-1))*self.cw[t]\n",
    "            if s > 0:\n",
    "                y=np.append(y,1)\n",
    "            else:\n",
    "                y=np.append(y,-1)\n",
    "        return y\n",
    "\n",
    "    def predict(self, X):\n",
    "        y = np.array([])\n",
    "        for x in X:\n",
    "            s = 0\n",
    "            for t in range(0,self.T):\n",
    "                s += self.weakModels[t].predict(x.reshape(1,-1))*self.cw[t]\n",
    "            if s > 0:\n",
    "                y=np.append(y,1)\n",
    "            else:\n",
    "                y=np.append(y,-1)\n",
    "        return y\n",
    "\n",
    "    def _weakPredict(self, X, t):\n",
    "        y = np.array([])\n",
    "        for x in X:\n",
    "            y=np.append(y,self.weakModels[t].predict(x.reshape(1,-1)))\n",
    "        return y\n",
    "\n",
    "    def _computeErr(w,m,y):\n",
    "        err = 0\n",
    "        for i in range(0,len(w)):\n",
    "            if(m[i]!=y[i]):\n",
    "                err+=w[i]\n",
    "        return err\n",
    "\n",
    "    def _updateWeights(self,w,m,y,t):\n",
    "        for i in range(0,len(y)):\n",
    "            w[i]=w[i]*math.exp((-self.cw[t])*y[i]*m[i])\n",
    "        return w\n",
    "\n",
    "    def classificationErr(y,Y):\n",
    "        return 1/2-(np.inner(y,Y)/(2*len(Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the implementation you are free to assume:\n",
    "- that the problem is a binary classification problem with labels in $\\{-1, +1\\}$.\n",
    "- that the weakModel can fit a weighted sample set by means of the call `weakModel.fit(X,y,sample_weight=w)` where `w` is a vector of length $|y|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation on the dataset loaded above and using an SVC with a polynomial kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration T: 0 weighted err:0.34049999999995745 Classification Error:0.49275\n",
      "Iteration T: 10 weighted err:0.3841514891774316 Classification Error:0.232875\n",
      "Iteration T: 20 weighted err:0.40228816966093855 Classification Error:0.17912499999999998\n",
      "Iteration T: 30 weighted err:0.4268418153431585 Classification Error:0.16599999999999998\n",
      "Iteration T: 40 weighted err:0.4290038536146344 Classification Error:0.153125\n",
      "Iteration T: 50 weighted err:0.4266811240897086 Classification Error:0.14300000000000002\n",
      "Iteration T: 60 weighted err:0.44617962543471773 Classification Error:0.1345\n",
      "Iteration T: 70 weighted err:0.4569049130463657 Classification Error:0.12637500000000002\n",
      "Iteration T: 80 weighted err:0.45466612909962023 Classification Error:0.12175000000000002\n",
      "Iteration T: 90 weighted err:0.41474681937805635 Classification Error:0.12087500000000001\n",
      "Error on training set:  0.11325000000000002\n",
      "Error on test set:  0.13824999999999998\n"
     ]
    }
   ],
   "source": [
    "def classificationErr(y,Y):\n",
    "    return 1/2-(np.inner(y,Y)/(2*len(Y)))\n",
    "\n",
    "weakModel = SVC(kernel=\"poly\", degree=3)\n",
    "adaboost = AdaBoost(weakModel, 100)\n",
    "\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "y_train_ = adaboost.predict(X_train)\n",
    "y_test_ = adaboost.predict(X_test)\n",
    "\n",
    "print(\"Error on training set: \", classificationErr(y_train_, y_train))\n",
    "print(\"Error on test set: \", classificationErr(y_test_, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and evaluate the AdaBoost performances as usual by calculating the classification error. \n",
    "\n",
    "**Note 1**:  \n",
    "since the labels are bound to be in ${+1, -1}$, the classification error can be easily computed as:\n",
    "$$\n",
    "   error(y,y') = \\frac{1}{2} - \\frac{y^T \\times y'}{2N},\n",
    "$$\n",
    "where $N$ is the total number of examples. The formula can be derived noticing that $y^T \\times y'$ calculates the number $N_c$ of examples correctly classified  minus the number $N_{\\bar c}$ of examples incorrectly classified. We have then $y^T \\times y' = N_c - N_{\\bar c}$ and by noticing that $N = N_c + N_{\\bar c}$:\n",
    "$$\n",
    "   N - y^T \\times y' = 2 N_{\\bar c} \\Rightarrow \\frac{N - y^T \\times y'}{2 N} = \\frac{N_{\\bar c}}{N} = error(y,y')\n",
    "$$\n",
    "\n",
    "**Note 2**:\n",
    "do not forget to deepcopy your base model before fitting it to the new data\n",
    "\n",
    "**Note 3**:\n",
    "The SVC model allows specifying weights, but it *does not* work well when weights are normalized (it works well when the weights are larger). The following class takes normalized weights and denormalize them before passing them to the SVC classifier:\n",
    "\n",
    "```python\n",
    "    class SVC_:\n",
    "        def __init__(self, kernel=\"rbf\", degree=\"3\"):\n",
    "            self.svc = SVC(kernel=kernel, degree=degree)\n",
    "\n",
    "        def fit(self, X,y,sample_weight=None):\n",
    "            if sample_weight is not None:\n",
    "                sample_weight = sample_weight * len(X)\n",
    "\n",
    "            self.svc.fit(X,y,sample_weight=sample_weight)\n",
    "            return self\n",
    "\n",
    "        def predict(self, X):\n",
    "            return self.svc.predict(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a weak learner to be used with the AdaBoost algorithm you just wrote. The weak learner that you will implement shall work as follows:\n",
    "\n",
    "- creates a random linear model $y(x) = \\mathbf{w} \\cdot \\mathbf{x} + t$ by generating the needed weight vector $\\mathbf{w}$ and $t$ at random; each weight shall be sampled from U(-1,1);\n",
    "- it evaluates the weighted loss $\\epsilon_t$ on the given dataset and flip the linear model if $\\epsilon_t > 0.5$\n",
    "- at prediction time it predicts +1 if $\\mathbf{x} \\cdot \\mathbf{w} > 0$ it predicts -1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLinearModel:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        return None\n",
    "\n",
    "    def loss(self, y, y_, w):\n",
    "        miss = [int(x) for x in (y_ != y)]\n",
    "\n",
    "        err = np.dot(w,miss) / sum(w)\n",
    "\n",
    "        if(err > 0.5):\n",
    "            self.w*-1\n",
    "\n",
    "        return None\n",
    "\n",
    "    def fit(self,X,y,sample_weight=None):\n",
    "        self.w = np.random.uniform(-1,1, X[0].size) \n",
    "        self.w = np.append(self.w, np.random.randint(-100,100)) # inizializzo i pesi a random qui invece che alla creazione, \n",
    "                                                                # dato che Adaboost si limita a copiare il classificatore\n",
    "        y_=np.zeros(y.size)\n",
    "\n",
    "        for i in range(0,len(X)):\n",
    "            y_[i]=self.predict(X[i])\n",
    "\n",
    "        self.loss(y,y_,sample_weight)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def predict(self,X):\n",
    "        X=np.append(X,1)\n",
    "        return 1 if np.dot(X,self.w) > 0 else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn an AdaBoost model using the RandomLinearModel weak learner printing every $K$ iterations the weighted error and the current error of the ensemble (you are free to choose $K$ so to make your output just frequent enough to let you know what is happening but without flooding the console with messages). Evaluate the training and test error of the final ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration T: 0 weighted err:0.48524999999993934 Classification Error:0.48525\n",
      "Iteration T: 1000 weighted err:0.4959852889194447 Classification Error:0.319\n",
      "Iteration T: 2000 weighted err:0.49999999999999956 Classification Error:0.248\n",
      "Iteration T: 3000 weighted err:0.49999999999999994 Classification Error:0.185875\n",
      "Iteration T: 4000 weighted err:0.4999999999999956 Classification Error:0.15749999999999997\n",
      "Iteration T: 5000 weighted err:0.5 Classification Error:0.13887500000000003\n",
      "Iteration T: 6000 weighted err:0.5000000000000018 Classification Error:0.12974999999999998\n",
      "Iteration T: 7000 weighted err:0.4843367639843449 Classification Error:0.12574999999999997\n",
      "Iteration T: 8000 weighted err:0.5 Classification Error:0.10925000000000001\n",
      "Iteration T: 9000 weighted err:0.5212232222526243 Classification Error:0.09687499999999999\n",
      "Error on training set:  0.08937499999999998\n",
      "Error on test set:  0.09225\n"
     ]
    }
   ],
   "source": [
    "def classificationErr(y,Y):\n",
    "    return 1/2-(np.inner(y,Y)/(2*len(Y)))\n",
    "\n",
    "rs = RandomLinearModel()\n",
    "a = AdaBoost(rs,10000)\n",
    "a.fit(X_train,y_train)\n",
    "\n",
    "y_train_ = a.predict(X_train)\n",
    "y_test_ = a.predict(X_test)\n",
    "\n",
    "print(\"Error on training set: \", classificationErr(y_train_, y_train))\n",
    "print(\"Error on test set: \", classificationErr(y_test_, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write few paragraphs about what you think about the experiment and about the results you obtained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trovo che AdaBoost sia un algoritmo di apprendimento assemble molto interessante, rispecchia l'idea dell'unione fa la forza, ogni classificatore aiuta a classificare correttamente una parte degli esempi, che non sono stati classificati correttamente dai suoi predecessori.\n",
    "\n",
    "Esercizio 1:\n",
    "In questo esercizio viene usato come classificatore debole, una SVM.\n",
    "Dai risulati si nota che l'errore di classificazione ensemble diminuisce rapidamente, mentre l'errore pesato del classificatore all'iterazione T sale o scende di poco.\n",
    "Questo fa capire che un classificatore da solo non sarebbe in grado di classificare in modo adeguato il dataset. Inoltre il fatto che l'errore pesato salga, fa pensare che il classificatore all'iterazione T, si sia specializzato a classificare solamente quegli esempi, che non venivano classificati correttamente dai classificatori generati in precedenza.\n",
    "\n",
    "Esercizio 2:\n",
    "In questo esercizio il modello utilizzato è molto semplice, si limita a invertire la pendenza dell'iperpiano ogni volta che classifica in modo sbagliato più di metà del dataset.\n",
    "La semplicità del modello la si nota anche dal fatto che l'errore pesato all'iterazione T resta sempre intorno a 0.5, quindi significa che riesce giusto a classificare un po meglio di una scelta completamente casuale.\n",
    "Dato che il modello è molto semplice, le iterazioni richieste per ottenere un risultato soddisfacente sono molte di più rispetto all'esercizio precedente con le SVM, però questo dimostra la potenza di Adaboost, non importa quanto il modello sia semplice se unito a tanti altri, alla fine riuscirà sempre a riprodurre i risultati che si otterrebbero con un modello più complesso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
